---
title: "Data Science with Neo4j: Predictions"
author: "Estefania Mancini"
date: "5/27/2020"
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

# Data Science with Neo4j
  
This course introduces you to using Neo4j as part of your Data Science and Machine Learning workflows. We’re going to learn how to do this with the help of the aminer.org citation dataset. This dataset contains papers, authors, and citations from DBLP – a computer science bibliography website. This course is intended for data scientists and data analysts. This self-paced training should take you three hours to complete if you perform all of the hands-on exercises in the course.

We have set up a discussion area in our Neo4j Community Site, if you run into problems in the course and need assistance. You should register on the community site where you can view other questions and answers for students taking our online training courses. The Neo4j Community Site is an excellent resource for answering many types of questions posed by other users of Neo4j.

There are four modules in this course. Most modules have hands-on exercises you should complete and all modules have a set of review questions at the end. The hands-on exercises in this course can be completed in a Neo4j Sandbox or in Neo4j Desktop. We have provided an estimate of how long each module should take you to complete if you perform the hands-on exercises.  

## Setting Up Your Development Environment
To configure this citation database, I followed this forum:
 < https://community.neo4j.com/t/download-datasets/11723/6 >
 
I replicate my local neo4j folder, and started a second neo4j machine to avoid messy. Then I follow this instructions:

* Here is the Citations dataset:
https://s3.amazonaws.com/neo4j-sandbox-usecase-datastores/v3_4/citations.db.zi
* Unzip the database zip file.
* Start the database.
* Stop the database.
* Copy the folder where you unzipped the database to the databases folder.
* (Delete the graph.db folder)
* Rename the to graph.db.
* Start the database.

Once it is up, you can connect using http://localhost:7474/browser/

## Import data using R:
I use this package: https://github.com/neo4j-rstats/neo4r

```{r Load, echo = TRUE}
library(neo4r)
con <- neo4j_api$new(
  url = "http://localhost:7474", 
  user = "neo4j", 
  password = ""
)
```


## Check connection and some specificities

```{r check1, echo = TRUE}
con$ping()
con$get_version()
con$get_constraints()
con$get_labels()
con$get_relationships()
con$get_index()
```
# The Link Prediction problem
Link Prediction has been around for a long time, but was popularised by a paper written by Jon Kleinberg and David Liben-Nowell in 2004, titled The Link Prediction Problem for Social Networks.
<https://www.cs.cornell.edu/home/kleinber/link-pred.pdf> 

Kleinberg and Liben-Nowell approach this problem from the perspective of social networks, asking this question:

Given a snapshot of a social network, can we infer which new interactions among its members are likely to occur in the near future?

We formalize this question as the Link Prediction problem, and develop approaches to Link Prediction based on measures for analyzing the “proximity” of nodes in a network.

For example, we could predict future associations between:

* People in a terrorist network
* Associations between molecules in a biology network
* Potential co-authorships in a citation network
* Interest in an artist or artwork

In each these examples, predicting a link means that we are predicting some future behaviour. For example in a citation network, we’re actually predicting the action of two people collaborating on a paper.

## Link Prediction Algorithms
Kleinberg and Liben-Nowell describe a set of methods that can be used for Link Prediction. These methods compute a score for a pair of nodes, where the score could be considered a measure of proximity or “similarity” between those nodes based on the graph topology. The closer two nodes are, the more likely there will be a relationship between them.

## Exercise 1:  Running Link Prediction algorithms
You will gain some experience running the Link Prediction algorithms. In the query edit pane of Neo4j Browser, execute the browser command: :play data-science-exercises and follow the instructions for the Link Prediction exercise.

## Applying Link Prediction Algorithms
Now that you have learned how to execute the link prediction algorithms, you will learn what to do with the results. There are two approaches:

* Using the measures directly: You can use the scores from the link prediction algorithms directly. With this approach you set a threshold value above which the algorithm would predict that a pair of nodes will have a link. For example, you might say that every pair of nodes that has a preferential attachment score above **3** would have a link, and any with **3 or less** would not.

* Supervised learning: You can take a supervised learning approach where you use the scores as features to train a binary classifier. The binary classifier then predicts whether a pair of nodes will have a link.

## Building a co-author graph
You will build an inferred graph of co-authors based on people collaborating on the same papers. You will store a property on the relationship indicating the year of their first collaboration.


```{r coauthors, echo = TRUE, eval=FALSE}
  'CALL apoc.periodic.iterate(
  "MATCH (a1)<-[:AUTHOR]-(paper)-[:AUTHOR]->(a2:Author)
   WITH a1, a2, paper
   ORDER BY a1, paper.year
   RETURN a1, a2, collect(paper)[0].year AS year, count(*) AS collaborations",
  "MERGE (a1)-[coauthor:CO_AUTHOR {year: year}]-(a2)
   SET coauthor.collaborations = collaborations", 
  {batchSize: 100})'   %>% call_neo4j(con)
```

Now that you have created a co-author graph, you need an approach that will allow you to predict future links (relationships) that will be created between people.

Once you have computed scores with this algorithms what should you do?

There are two main approaches that one can take:
* Using the measures directly: You can use the scores from the link predictions directly, specifying a threshold value above which we predict that a link will be created between two nodes.

* Supervised learning: You can take a supervised learning approach where you use the scores as features to train a binary classifier. The binary classifier then predicts whether a pair of nodes will have a link.

* Train and test datasets: Next, you must create the train and test datasets on which you can build, and then evaluate a model.

* Positive examples: The tricky thing when working with graph data is that you cannot just randomly split the data, as this could lead to data leakage.

Data leakage can occur when data outside of your training data is inadvertently used to create your model. This can easily happen when working with graphs because pairs of nodes in the training set may be connected to those in the test set.

When you compute link prediction measures over that training set the measures computed contain information from the test set that you will later evaluate the model against. Instead, you need to split the graph into training and test sub graphs. If the graph has a concept of time, things are easier as you can split the graph at a point in time. The training set will be from before the time, the test set after. This is still not a perfect solution and you must ensure that the general network structure in the training and test sub graphs is similar. 

Subsequently, pairs of nodes in our train and test datasets will have relationships between them. They will be the positive examples in your machine learning model.

Because the citation graph contains times, you can create train and test graphs by splitting the data on a particular year. Next, you must determine what year that should be. Determine the distribution of the first year that co-authors collaborated:

```{r coauthors2, echo = TRUE}
year<-as.data.frame('MATCH p=()-[r:CO_AUTHOR]->() 
WITH r.year AS year, count(*) AS count
ORDER BY year
RETURN toString(year) AS year, count'%>% call_neo4j(con))
colnames(year)<-c("year", "count")
ggplot(year, aes(x=year, y=count)) +
geom_point()+ 
theme_classic()+
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 5))
```


It looks like 2006 would act as a good year for splitting the data. All co-authorships from 2005 and earlier as our train graph, and everything from 2006 onwards as the test graph. Create explicit CO_AUTHOR_EARLY and CO_AUTHOR_LATE relationships in the graph based on that year. 

```{r coauthorsEarly, echo = TRUE, eval=FALSE}
'MATCH (a)-[r:CO_AUTHOR]->(b) 
where r.year < 2006
MERGE (a)-[:CO_AUTHOR_EARLY {year: r.year}]-(b)'
 %>% call_neo4j(con)
```

```{r coauthorsLate, echo = TRUE, eval=FALSE}
'MATCH (a)-[r:CO_AUTHOR]->(b) 
where r.year > 2006
MERGE (a)-[:CO_AUTHOR_LATE {year: r.year}]-(b)'
 %>% call_neo4j(con)
```

* Determine how many co-author relationship you have in each of these sub graphs:

```{r coauthorsEarlyCounts, echo = TRUE}
nearly<-'MATCH ()-[:CO_AUTHOR_EARLY]->() RETURN count(*) AS count'  %>% call_neo4j(con)
nearly
```

```{r coauthorsLateCounts, echo = TRUE}
nlate<-'MATCH ()-[:CO_AUTHOR_LATE]->()
RETURN count(*) AS count'  %>% call_neo4j(con)
nlate
```

## Negative examples
The simplest approach is to use all pair of nodes that don’t have a relationship. The problem with this approach is that there are significantly more examples of pairs of nodes that don’t have a relationship than there are pairs of nodes that do. 

The maximum number of negative examples is equal to:

$$ \# negative examples = (\# nodes)^2 - (\# relationships) - (\# nodes) $$

If you were to use all of these negative examples in your training set, you would have a massive class imbalance - there are many negative examples and relatively few positive ones.

A model trained using data that is this imbalanced will achieve very high accuracy by predicting that any pair of nodes don’t have a relationship between them, which is not quite what we want!

Now you are ready to build the train and test datasets based on the train and test sub graphs that you created.

The positive examples will be taken directly from the graph.
The negative examples will be found by looking for people who are 2 or 3 hops away from each other, excluding those that have already collaborated. You will then down sample those examples to equal the size of the positive examples.

You need to reduce the number of negative examples. An approach described in several link prediction papers is to use pairs of nodes that are a specific number of hops away from each other.

This will significantly reduce the number of negative examples, although there will still be a lot more negative examples than positive.

To solve this problem, you either need to down sample the negative examples or up sample the positive examples.

You will take the down sampling approach. 

Now you are ready to build the train and test datasets based on the train and test sub graphs that you created.

The positive examples will be taken directly from the graph.
The negative examples will be found by looking for people who are 2 or 3 hops away from each other, excluding those that have already collaborated. You will then down sample those examples to equal the size of the positive examples.

```{r training, echo = TRUE, eval=FALSE}
train_existing_links<-as.data.frame('MATCH (author:Author)-[:CO_AUTHOR_EARLY]->(other:Author)
RETURN id(author) AS node1, id(other) AS node2, 1 AS label' %>% call_neo4j(con))
train_missing_links = as.data.frame('MATCH (author:Author)
WHERE (author)-[:CO_AUTHOR_EARLY]-()
MATCH (author)-[:CO_AUTHOR_EARLY*2..3]-(other)
WHERE not((author)-[:CO_AUTHOR_EARLY]-(other))
RETURN id(author) AS node1, id(other) AS node2, 0 AS label' %>% call_neo4j(con))
```

```{r test, echo = TRUE, eval=FALSE}
test_existing_links = as.data.frame('
MATCH (author:Author)-[:CO_AUTHOR_LATE]->(other:Author)
RETURN id(author) AS node1, id(other) AS node2, 1 AS label'
%>% call_neo4j(con))

test_missing_links = as.data.frame(
  'MATCH (author:Author)
WHERE (author)-[:CO_AUTHOR_LATE]-()
MATCH (author)-[:CO_AUTHOR_LATE*2..3]-(other)
WHERE not((author)-[:CO_AUTHOR_LATE]-(other))
RETURN id(author) AS node1, id(other) AS node2, 0 AS label'
%>% call_neo4j(con))
test_missing_links = test_missing_links[!is.duplicated(test_missing_links),]
```

Combinar los dataframes y correr un algoritmo de clasificacion: ejemplo random forrest
```{r dftest, echo = TRUE, eval=FALSE}

```

## Choosing a machine learning algorithm
Next, you will create a machine learning pipeline based on a random forest classifier. This method is well suited as this data set will be comprised of a mix of strong and weak features. While the weak features will sometimes be helpful, the random forest method will ensure that you don’t create a model that only fits the training data.

## Generating graphy features
* Start by creating a simple model that tries to predict whether two authors will have a future collaboration based on features extracted from common authors, preferential attachment, and the total union of neighbors.

* Next, you will build a model based on these graphy features. You will start by just using one of the features - common neighbors.

* builds a random forest model, evaluates it against the test dataset, and then indicates which of the features had the most importance in the model.

* You need to evaluate the model. You will compute its accuracy, precision, and recall. Then, you will return the importance of each feature used in the model. 

The scores for accuracy and precision are adequate, but the recall is not very good. What happens if you include preferential attachment and total neighbors as well?

* Common Neighbors is the dominant feature, but including the two other features has improved the accuracy and recall of the model. Next, you will add some new features that are generated from graph algorithms.

## Triangles and The Clustering Coefficient
Start by running the triangle count algorithm over the test and train sub-graphs. This algorithm will return the number of triangles that each node forms, as well as each node's clustering coefficient. **The clustering coefficient of a node indicates the likelihood that its neighbors are also connected.** 
Es la habilidad de generar comunidad -> nodos mas importantes, son aquellos que generan comunidad.

The Triangle Count algorithm counts the number of triangles for each node in the graph. A triangle is a set of three nodes where each node has a relationship to the other two. In graph theory terminology, this is sometimes referred to as a 3-clique. The Triangle Count algorithm in the GDS library only finds triangles in undirected graphs.

Triangle counting has gained popularity in social network analysis, where it is used to detect communities and measure the cohesiveness of those communities. 

It can also be used to determine the stability of a graph, and is often used as part of the computation of network indices, such as clustering coefficients. 

The Triangle Count algorithm is also used to compute the Local Clustering Coefficient.

Triangle count and clustering coefficient have been shown to be useful as features for classifying a given website as spam, or non-spam, content. This is described in "Efficient Semi-streaming Algorithms for Local Triangle Counting in Massive Graphs".

The coefficient features have not added much to our model, but the triangles are useful. Next you will see if Community Detection algorithms can help improve the model.

# Community Detection

Community Detection algorithms evaluate how a group is clustered or partitioned. *Nodes are considered more similar to nodes that fall in their community than to nodes in other communities*.

You will run two Community Detection algorithms over the train and test sub-graphs - Label Propagation and Louvain. 

*Label Propagation: 

*The Louvain algorithm returns intermediate communities, which are useful for finding fine grained communities (
comunidades granuladas) that exist in a graph. You will add a property to each node containing the community revealed on the first iteration of the algorithm:


